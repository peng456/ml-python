# ml-python
python 机器学习实战。边看边写记录。

而且 这是一个deadline !!! 

干什么都要有输出。记录，即使只是一个简简单单的 手写，重复，也是一种输出。



Python 机器学习实战

数据挖掘十大算法  
1、C4.5决策树  
2、K-均值（K-mean）  
3、支持向量机（SVM）  
4、Apriori  
5、最大期望算法（EM）  
6、PageRank  
7、AdaBoost  
8、kNN(k-近邻算法)  
9、朴素贝叶斯算法（NB）  
10、分类回归树（CART）算法  

本书结构

第一部分 ： 分类
监督学习一般使用两种类型的目标变量 :标称型和数值型。  


第一章：机器学习基础  

本章内容  
机器学习的简单概述  
机器学习的主要任务  
学习机器学习的原因  
Python 语言的优势  

始终觉得机器学习是统计学  

已经应用于多个领域。  
1、机器学习==》 搜索引擎  
2、垃圾邮件过滤  
￼

基于以前的统计知识  

机器学习： 人脸识别，手写数字识别、垃圾邮件过滤 、亚马逊公司的产品推荐


什么是机器学习：：？  
机器学习就是把无序的数据转化成有用的知识  

机器学习横跨计算机科学、工程技术 、统计学十多个学科  

1.1.1传感器  和海量数据  
海量数据 时代已经到来  
1.1.2 机器学习很重要  
发达国家 体力劳动   ==转化=》脑力劳动

1.2 关键术语  

鸟分类例子  

机器学习的主要任务  就是分类  
有很多机器学习算法非常善于分类  

数据 +  算法 ==训练=》模型  

训练数据 、测试数据  


1.3 机器学习的主要任务  
另一项任务： 回归  ===》主要用于 预测 数据数值数据   

比如： 数据拟合曲线。最有拟合曲线。  

分类、回归属于监督学习。  
为什么叫监督学习：：  这类算法 必须 知道 预测什么。==》即目标变量的分类信息  


无监督学习： 数据无分类，也不会给定目标值。  

需要自己找出。分类规律。自己知道分类的存在  

无监督学习： 聚类（数据分类） && 密度估计（寻找描述数据统计值的过程）  

好处===》 减少数据特征的 维度  


监督学习的用途  
K-近邻算法						线性回归  
朴素贝叶斯算法			局部回归  

支持向量机					Ridge 回归  

决策树						Lasso 最小回归系数估计  


1.4 如何选择何时的算法  


面临问题： 
	1、使用机器学习的目的，，用算法完成何种任务  
 	2、需要分析 or 收集的 数据是什么？？？


预测目标变量的值===》监督学习算法  ====》 确定目标变量 类型  离散               ===》分类算法  
			连续数值     ===》回归算法


不想预测目标变量的值===》 无监督学习算法==》数据是否需要划分离散的组===》唯一的需求===》聚类算法  
   									&&估计数据与每个组分组的相似程度===》密度估计算法


其次：： 充分了解数据==>越充分==》越符合需求的算法


了解数据的几个指标：
	
	1、特征值：离散型 or 连续型  
  2、是否缺失值、缺失原因  
  3、是否存在异常值  
  4、某特征值频率如何（是否罕见）
	
===>可以缩短 算法的选择范围


1.5开发机器学习应用程序的步骤

收集数据—》准备数据输入（格式整理）—》分析输入数据—》训练算法—》测试算法 ——>得到模型—》使用算法


1.6python 语言的优势

语法清晰、易于操作纯文本文件、使用广泛（大量开发文档）

非常多的库，免费

缺点==》性能不足  ==》Cpython
		

1.7 NumPy 函数库基础


独立模块


from numpy import *

数组、矩阵

array	 matrix

randMat = mat(random.rand(4,4))

randMat.I  矩阵逆运算

1.8 小结
	机器学习的应用前景几乎是无限的。！！！


第二章：k-临近算法  

有数据，且标注了数据

选择

https://www.cnblogs.com/ybjourney/p/4702562.html

1、求被测试数据   与 各个已知数据  之间的距离 （距离   = 相似度）=====》按照距离排序（由近到远）===》取离被测试数据最近的K个点===》 统计 K个数据点中 各自属性的频率  ===》 各个属性频率高的 为  被测试数据的 属性值

1、这个第一步的运算量好大。当数据集 的样例越大，运算O(n)的。。。。。。

2、第一步距离算法，也是个技术活。  
	欧氏距离或曼哈顿距离
￼
这个距离。是什么意思？

坐标点？？？  那属性值 跟这个什么 关系？？？


3、k值的选择。（是个技术活）
￼
K 的选值不同 绿色圆形  被预测的 结果完全不同。。。。。感觉不靠谱

与自己的业务相比。这运算量TMD，感觉上了好几个层级。

实现。

看代码		==》 


// 离散数值  两个维度 上的值  
    group = array([[1.0,2.0],[1.2,0.1],[0.1,1.4],[0.3,3.5]])
// 标记 两个维度最终真值  
    labels = ['A','A','B','B']
    

###通过KNN进行分类
def classify(input,dataSet,label,k):
    dataSize = dataSet.shape[0]
    ####计算欧式距离
    diff = tile(input,(dataSize,1)) - dataSet
    sqdiff = diff ** 2
    squareDist = sum(sqdiff,axis = 1)###行向量分别相加，从而得到新的一个行向量
    dist = squareDist ** 0.5

    ##对距离进行排序
    sortedDistIndex = argsort(dist)##argsort()根据元素的值从大到小对元素进行排序，返回下标

    classCount={}
    for i in range(k):
        voteLabel = label[sortedDistIndex[i]]
        ###对选取的K个样本所属的类别个数进行统计
        classCount[voteLabel] = classCount.get(voteLabel,0) + 1
    ###选取出现的类别次数最多的类别
    maxCount = 0
    for key,value in classCount.items():
        if value > maxCount:
            maxCount = value
            classes = key

    return classes

python test.py  每次运行答案不一样
￼

很大工作是  数据预处理

把属性处理成 离散值

n个属性 ===》n维的离散值 必读 长度x,宽y,高z,颜色，e …….
（x,y,z,e, …….）    ====> 已有数据对应一个固定结果红色长方体

KNN 。取离最近的。统计K中结果 频率最高的，为 预测值



距离的计算  ===》 数据预处理 ==》 离散值

点(1, 0, 0, 1)与(7, 6, 9, 4)之间的距离计算为:
￼
均方差

===》 所有结果排序 ==》 取前K个  ==》 统计 频率 ===》 频率最高的 为结果

===》 如果  多维呢？  ===》 统计 各个维度 的频率，取各自最高的么？？？

K-近邻  ===》约会网站  配对 效果提升


第三章：决策树
第四章：基于概率论的方法：朴素贝叶斯
第五章：Logistic 回归
第六集：支持向量机
第七章：利用AdaBoot元算法提高分类

第二部分： 利用回归预测数值型数据
第八章：预测数值型数据：回归
第九章：树回归

第三部分：无监督学习  
第十章：利用K-均值聚类算法对未标注数据分组  
第十一章：使用Apriori算法进行关联分析  
第十二章：使用FP-growth 算法来高效发现频繁项集  

第四部分：其他工具  
第十三章：利用PCA来简化数据  

第十四章：利用SVD简化数据  

第十五章： 大数据与MapReduce  


附录A Python 入门  

附录B 线性代数  

附录C 概率论复习  

附录D 资源  

索引  

版权声明  




